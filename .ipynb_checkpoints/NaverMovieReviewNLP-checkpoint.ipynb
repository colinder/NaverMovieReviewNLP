{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU 연동 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 6715643093103495729]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-4a3f57d5652e>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8f570ef3ac05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'XLA_GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_visible_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'XLA_GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_gpu_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('XLA_GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'XLA_GPU')\n",
    "\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55vBm4qYrBz0"
   },
   "source": [
    "# Naver 영화 리뷰 감정분석\n",
    "데이터 셋은 https://github.com/e9t/nsmc 의 자료를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVMyIJHC0FJ-",
    "outputId": "7d6cf2e5-bc0c-44ce-a222-7569a11d8908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 개수:  200000\n",
      "훈련 데이터 개수: 150000\n",
      "검증 데이터 개수: 50000\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "## data 가져오기\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"total_data.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"train_data.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"test_data.txt\")\n",
    "\n",
    "total_data = pd.read_table(\"total_data.txt\")            # read_table시 자동으로 strip() 적용\n",
    "train_data = pd.read_table(\"train_data.txt\")\n",
    "test_data = pd.read_table(\"test_data.txt\")\n",
    "\n",
    "print(\"총 데이터 개수: \", len(total_data))\n",
    "print(\"훈련 데이터 개수:\", len(train_data))\n",
    "print(\"검증 데이터 개수:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cr6h-qZ8rr81"
   },
   "source": [
    "## 데이터 __확인__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "PJyUyHcc9HSh",
    "outputId": "ebb2a5b6-1641-4a69-bbdb-f6e74e397797"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리 진행\n",
    "데이터의 결측값, 이상값, 빈값, 중복 등을 처리하여 정제된 데이터 셋으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. 정규표현식 적용\n",
    "train_data[\"document\"] = train_data['document'].str.replace(\"[^0-9^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True).str.replace(\"[ㄱ-ㅎㅏ-ㅣ]\", \"\", regex=True).str.replace(\"^\", \"\", regex=False)\n",
    "test_data[\"document\"] = test_data['document'].str.replace(\"[^0-9^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True).str.replace(\"[ㄱ-ㅎㅏ-ㅣ]\", \"\", regex=True).str.replace(\"^\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████▋                                                | 46440/150000 [14:20:15<1:38:53, 17.45it/s]"
     ]
    }
   ],
   "source": [
    "# 맞춤법 및 띄어쓰기 교정\n",
    "from hanspell import spell_checker\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "correction_document = []\n",
    "for i in tqdm(range(train_data.shape[0])):\n",
    "    try:\n",
    "        cor_doc = spell_checker.check(train_data['document'][i])\n",
    "        correction_document.append(cor_doc.checked)\n",
    "    except:\n",
    "        correction_document.append(\"\")\n",
    "    # 106분 소요\n",
    "\n",
    "train_data['document'] = pd.DataFrame(correction_document)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맞춤법 교정 - 계속\n",
    "from tqdm import tqdm        # p-var\n",
    "\n",
    "from hanspell import spell_checker\n",
    "\n",
    "correction_document = []\n",
    "for i in tqdm(range(test_data.shape[0])):\n",
    "    try:\n",
    "        cor_doc = spell_checker.check(test_data['document'][i])\n",
    "        correction_document.append(cor_doc.checked)\n",
    "    except:\n",
    "        correction_document.append(\"\")\n",
    "    # 35분 소요\n",
    "\n",
    "test_data['document'] = pd.DataFrame(correction_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Id 별 중복되는 document 확인 및 삭제\n",
    "# 다만, id는 다르지만 중복된 리뷰라면 개개인의 의견이니 제거 하지 않는다.\n",
    "train_data[\"id\"].is_unique        # is_unique ; res => boolean True - 중복 없음 / False - 중복 있음  # True\n",
    "test_data['id'].is_unique         # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "## train_data 진행\n",
    "file = 'cor_train_data.txt'\n",
    "\n",
    "if os.path.isfile(file):\n",
    "    print(f'{file} 파일이 존재합니다. 불러옵니다.')\n",
    "    with open('cor_train_data.txt', 'rb') as f:\n",
    "        cor_train_data = pickle.load(f)\n",
    "        train_data['document'] = pd.DataFrame(cor_train_data)\n",
    "        print(\"파일을 불러왔습니다.\")\n",
    "else:\n",
    "    with open(\"cor_train_data.txt\", 'wb') as F:\n",
    "        pickle.dump(correction_document, F, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"파일을 생성하였습니다.\")\n",
    "        \n",
    "        \n",
    "## test_data 진행\n",
    "file = 'cor_test_data.txt'\n",
    "\n",
    "if os.path.isfile(file):\n",
    "    print(f'{file} 파일이 존재합니다. 불러옵니다.')\n",
    "    with open('cor_test_data.txt', 'rb') as f:\n",
    "        cor_test_data = pickle.load(f)\n",
    "        test_data['document'] = pd.DataFrame(cor_test_data)\n",
    "        print(\"파일을 불러왔습니다.\")\n",
    "else:\n",
    "    with open(\"cor_test_data.txt\", 'wb') as F:\n",
    "        pickle.dump(correction_document, F, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"파일을 생성하였습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값(NaN, Null) 제거\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값(NaN, Null) 제거\n",
    "train_data.dropna(inplace = True)\n",
    "test_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈값 확인 \n",
    "train_drop = [ i for i, v in enumerate(train_data['document']) if str(v).strip() == \"\" ]\n",
    "test_drop = [ i for i, v in enumerate(test_data['document']) if str(v).strip() == \"\" ]\n",
    "\n",
    "train_data.loc[train_data.index[train_drop]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈값 제거\n",
    "train_data.drop(train_drop, inplace=True)\n",
    "test_data.drop(test_drop, inplace=True)\n",
    "\n",
    "# 인덱스 재정렬\n",
    "train_data.reset_index(drop=True,  inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NXWvD_4-xJ6"
   },
   "source": [
    "# 3. 데이터 코드화\n",
    "\n",
    "(LSTM 분석을 위해서는 벡터화된 데이터셋이 필요함.)</br>\n",
    "토큰화는 Kkma, Komoran 두가지로 테스트 예정</br>\n",
    "둘 중 프로젝트에 더 적합한 모습을 보이는 라이브러리로 진행을 위한 테스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNpi6jEtIRGD",
    "outputId": "306e641a-9f58-40e5-ed0f-2038a19bb168"
   },
   "outputs": [],
   "source": [
    "import jpype\n",
    "\n",
    "from konlpy.tag import Kkma, Hannanum, Komoran, Okt\n",
    "\n",
    "Kkma = Kkma()\n",
    "Komoran = Komoran()\n",
    "Okt = Okt()\n",
    "Hannanum = Hannanum()\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"=\"*100)\n",
    "    print(train_data['document'][i])\n",
    "    print(\"Kkma\", Kkma.morphs(train_data['document'][i]))\n",
    "    print(\"Komran\",Komoran.morphs(train_data['document'][i]))\n",
    "    print(\"Okt\", Okt.morphs(train_data['document'][i], norm=True))   # norm: 정규화 처리 / stem: 각 단어에서 어간을 추출\n",
    "    print(\"Hannanum\", Hannanum.morphs(train_data['document'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mWta1oTTXKQ"
   },
   "source": [
    "_테스트 결과_<br>\n",
    "맞춤법과 띄어쓰기가 보정된 문장을 단어로 분리하는 경우 전체적으로 비슷한 성능을 보임<br>\n",
    "\n",
    "\n",
    "다만 속도에서 [독보적으로 빠른](https://mr-doosun.tistory.com/22) **Okt**를 사용하는 것이 적절하다고 판단 \n",
    "\n",
    "\n",
    "\n",
    "## 문장 → 단어로 분할 / 불용어 사전 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bU5uzZImVKT"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','걍','과', '의', '도','을', '것', '를','으로','자','에','와','한','하다',\n",
    "             'ㄴ', '다','에서', '하는', '나', '자', '고', '이다','수','데','인데']\n",
    "\n",
    "# 품사 태깅\n",
    "tagged_train_data = []\n",
    "train_error_sentence = []\n",
    "\n",
    "for i in tqdm(range(train_data.shape[0])):\n",
    "    try:\n",
    "        temp_x = Okt.morphs(train_data['document'][i], norm=True)                 # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords]   # 불용어 제거\n",
    "        tagged_train_data.append(temp_x)\n",
    "    except:\n",
    "        tagged_train_data.append([])\n",
    "        train_error_sentence.append(i)\n",
    "\n",
    "print(\"훈련데이터 에러 발생 수:\", len(train_error_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "file = 'tagged_train_data.txt'\n",
    "\n",
    "if os.path.isfile(file):\n",
    "    print('파일이 존재합니다. 불러옵니다')\n",
    "    with open('tagged_train_data.txt', 'rb') as f:\n",
    "        tagged_train_data = pickle.load(f)\n",
    "        print(\"불러오기 완료.\")\n",
    "else:\n",
    "    with open(\"tagged_train_data.txt\", 'wb') as F:\n",
    "        pickle.dump(tagged_train_data, F, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"파일을 생성하였습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm        # p-var\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','걍','과', '의', '도','을', '것', '를','으로','자','에','와','한','하다', \n",
    "             'ㄴ', '다','에서', '하는', '나', '자', '고', '이다','수','데','인데']   \n",
    "\n",
    "# 품사 태깅\n",
    "tagged_test_data = []\n",
    "test_error_sentence = []\n",
    "\n",
    "for i in tqdm(range(test_data.shape[0])):\n",
    "    try:\n",
    "        temp_x = Okt.morphs(test_data['document'][i], norm=True)                 # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords]   # 불용어 제거\n",
    "        tagged_test_data.append(temp_x)\n",
    "    except:\n",
    "        tagged_test_data.append([])\n",
    "        test_error_sentence.append(i)\n",
    "\n",
    "print(\"훈련데이터 에러 발생 수:\", len(test_error_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "file = 'tagged_test_data.txt'\n",
    "\n",
    "if os.path.isfile(file):\n",
    "    print('파일이 존재합니다. 불러옵니다')\n",
    "    with open('tagged_test_data.txt', 'rb') as f:\n",
    "        tagged_test_data = pickle.load(f)\n",
    "        print(\"불러오기 완료.\")\n",
    "else:\n",
    "    with open(\"tagged_test_data.txt\", 'wb') as F:\n",
    "        pickle.dump(tagged_test_data, F, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"파일을 생성하였습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기계가 텍스트를 숫자로 처리할 수 있도록 정수 인코딩을 진행\n",
    "<br>\n",
    "다만 태깅한 품사를 모두 사용해서 학습한다면, 1, 2회 사용된 <br>\n",
    "즉 너무 적은 단위로 사용된 단어들까지 학습된다. <br>\n",
    "하면 데이터 분석에 효율성이 떨어질 수 있다. (혹은 과적합)<br>\n",
    "하여 자주 사용된 단어(3회 이상)만 추려서 인공신경망 학습에 사용될 수 있도록<br>\n",
    "3회 이하 사용된 단어들은 제거 한다. \n",
    "\n",
    "#### 3회 이하로 사용된 단어들이 얼마나 있는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선 정수 인코딩 진행\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer_maxlen = Tokenizer()\n",
    "tokenizer_maxlen.fit_on_texts(tagged_train_data)\n",
    "\n",
    "print(tokenizer_maxlen.word_index)  # word_index => {'하': 1, 'ㄴ': 2, '영화': 3, '다': 4, '고': 5, ... ✨자주 사용된 단어가 먼저 나열\n",
    "                                    # word_counts => OrderedDict([('아', 154), ('더빙', 5), ('진짜', 69) ...\n",
    "                                    # word_docs => defaultdict(<class 'int'>, {'목소리': 2, '짜증나': 8, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 4\n",
    "total_cnt = len(tokenizer_maxlen.word_index)                # 모든 토큰의 수\n",
    "rare_cnt = 0                                                # 3번 이하 등장 단어 개수\n",
    "total_freq = 0                                              # 토근화된 단어의 총합\n",
    "rare_freq = 0                                               # 토근화된 단어 중 3번 이하 사용된 단어의 총합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer_maxlen.word_counts.items():       # word_counts: 해당 단어의 빈도수를 반환\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if value < threshold:\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 2번 이하인 희귀 단어의 수:', rare_cnt)\n",
    "print(\"전체 품사 집합에서 2회 이하 사용된 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "voca_size = total_cnt - rare_cnt + 1\n",
    "print(f'즉, 총 단어의 수는 {total_cnt}개 / 그 중 2회 이하 사용된 단어의 수는 {rare_cnt}')\n",
    "print(\"2회 이하 사용된 단어를 제거한 == 남길 단어(Embedding)의 사이즈:\", voca_size - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩 할 단어 voca_size 를 확인</n>\n",
    "\n",
    "### *본 데이터 임베딩 진행\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=voca_size)                           # Tokenizer(vaca_size) => 토근화 진행하면서 max_embadding(100, 12, 4 ...)를 설정\n",
    "# 학습 데이터 \n",
    "tokenizer.fit_on_texts(tagged_train_data)                            # fit_on_texts => Updates internal vocabulary based on a list of texts.\n",
    "enbadded_train_data = tokenizer.texts_to_sequences(tagged_train_data)    # texts_to_sequences => Transforms each text in texts to a sequence of integers.\n",
    "\n",
    "# 검증 데이터 \n",
    "tokenizer.fit_on_texts(tagged_test_data)                             # fit_on_texts => Updates internal vocabulary based on a list of texts.\n",
    "embadded_test_data = tokenizer.texts_to_sequences(tagged_test_data)\n",
    "\n",
    "len(enbadded_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트를 위해 데이터 분리 및 임베딩 데이터 추가\n",
    "ex_train_data = train_data\n",
    "ex_test_data = test_data\n",
    "\n",
    "ex_train_data.insert(ex_train_data.shape[1], \"token\", enbadded_train_data)\n",
    "ex_test_data.insert(ex_train_data.shape[1], \"token\", embadded_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_train_data.to_csv(\"ex_train_data.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 리스트 행 확인\n",
    "train_drop = [i for i, v in enumerate(ex_train_data['token']) if v == []]\n",
    "test_drop = [i for i, v in enumerate(ex_test_data['token']) if v == []]\n",
    "\n",
    "print(len(train_drop))\n",
    "print(len(test_drop))\n",
    "print(train_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 리스트 행 제거 및 확인\n",
    "ex_train_data.drop(train_drop, inplace=True)\n",
    "ex_test_data.drop(test_drop, inplace=True)\n",
    "\n",
    "ex_train_data.reset_index(drop=True, inplace=True)\n",
    "ex_test_data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_test_data.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습을 위한 sequence padding 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in refine_train_data[\"token\"]))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, refine_train_data[\"token\"]))/len(refine_train_data[\"token\"]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist([len(s) for s in refine_train_data[\"token\"]], bins=60)\n",
    "plt.xlabel('length ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def belong(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n",
    "\n",
    "max_len = 30\n",
    "for max_len in range(30, 60, 5):\n",
    "    belong(max_len, refine_data[\"token\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding 길이 설정 후 => train_x 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "train_x = sequence.pad_sequences(refine_data['token'], maxlen=40)\n",
    "\n",
    "print(train_x[:3], type(traind_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## refine_train_data에서 라벨값을 추려 => train_label 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = refine_data['label'].to_numpy()\n",
    "print(train_label[:3], type(train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_x 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = sequence.pad_sequences(refine_test_data['token'], maxlen=40)\n",
    "print(test_x[:3], type(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## refine_test_data에서 라벨값을 추려 => test_label 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = refine_test_data['label'].to_numpy()\n",
    "print(test_label[:3], type(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(voca_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(train_x, train_label, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
    "\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(test_x, test_label)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "  new_sentence = Kkma.morphs(new_sentence) # 토큰화\n",
    "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "  pad_new = sequence.pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "  score = float(loaded_model.predict(pad_new)) # 예측\n",
    "  if(score > 0.5):\n",
    "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "  else:\n",
    "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))\n",
    "    \n",
    "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이딴게 영화냐 ㅉㅉ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('감독 뭐하는 놈이냐?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZwk3gkZOGkPxFbpky+WNo",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Naver 영화 리뷰 감정분석.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
