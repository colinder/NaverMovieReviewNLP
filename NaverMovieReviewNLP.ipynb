{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU 연동 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15281247317931586966,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 4409662798050905327\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 3048629863\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 411006660113267433\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:02:00.0, compute capability: 7.5\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 14068690066283100003\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-4a3f57d5652e>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('XLA_GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'XLA_GPU')\n",
    "\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55vBm4qYrBz0"
   },
   "source": [
    "# Naver 영화 리뷰 감정분석\n",
    "데이터 셋은 https://github.com/e9t/nsmc 의 자료를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVMyIJHC0FJ-",
    "outputId": "7d6cf2e5-bc0c-44ce-a222-7569a11d8908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 개수:  200000\n",
      "훈련 데이터 개수: 150000\n",
      "검증 데이터 개수: 50000\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "## data 가져오기\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"total_data.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"train_data.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"test_data.txt\")\n",
    "\n",
    "total_data = pd.read_table(\"total_data.txt\")            # read_table시 자동으로 strip() 적용\n",
    "train_data = pd.read_table(\"train_data.txt\")\n",
    "test_data = pd.read_table(\"test_data.txt\")\n",
    "\n",
    "print(\"총 데이터 개수: \", len(total_data))\n",
    "print(\"훈련 데이터 개수:\", len(train_data))\n",
    "print(\"검증 데이터 개수:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cr6h-qZ8rr81"
   },
   "source": [
    "## 데이터 __확인__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "PJyUyHcc9HSh",
    "outputId": "ebb2a5b6-1641-4a69-bbdb-f6e74e397797"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리 진행\n",
    "데이터의 결측값, 이상값, 빈값, 중복 등을 처리하여 정제된 데이터 셋으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. 정규표현식 적용\n",
    "train_data[\"document\"] = train_data['document'].str.replace(\"[^0-9^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True).str.replace(\"[ㄱ-ㅎㅏ-ㅣ]\", \"\", regex=True).str.replace(\"^\", \"\", regex=False)\n",
    "test_data[\"document\"] = test_data['document'].str.replace(\"[^0-9^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True).str.replace(\"[ㄱ-ㅎㅏ-ㅣ]\", \"\", regex=True).str.replace(\"^\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 150000/150000 [1:47:55<00:00, 23.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙 진짜 짜증 나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠포스터보고 초등학생영화 줄 오버 연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 솔직히 재미는 없다 평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이먼 페그의 익살스러운 연기가 돋보였던 영화 스파이더맨에서 늙어 보이기만 했던 커...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>6222902</td>\n",
       "      <td>인간이 문제지 소는 뭔 죄인가</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>8549745</td>\n",
       "      <td>평점이 너무 낮아서</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>9311800</td>\n",
       "      <td>이게 뭐요 한국인은 거들먹거리고 필리핀 혼혈은 착하다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>2376369</td>\n",
       "      <td>청춘 영화의 최고봉 방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>9619869</td>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        9976970                                 아 더빙 진짜 짜증 나네요 목소리      0\n",
       "1        3819312                    흠포스터보고 초등학생영화 줄 오버 연기조차 가볍지 않구나      1\n",
       "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3        9045019                         교도소 이야기구먼 솔직히 재미는 없다 평점 조정      0\n",
       "4        6483659  사이먼 페그의 익살스러운 연기가 돋보였던 영화 스파이더맨에서 늙어 보이기만 했던 커...      1\n",
       "...          ...                                                ...    ...\n",
       "149995   6222902                                   인간이 문제지 소는 뭔 죄인가      0\n",
       "149996   8549745                                         평점이 너무 낮아서      1\n",
       "149997   9311800                      이게 뭐요 한국인은 거들먹거리고 필리핀 혼혈은 착하다      0\n",
       "149998   2376369                        청춘 영화의 최고봉 방황과 우울했던 날들의 자화상      1\n",
       "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[150000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 맞춤법 및 띄어쓰기 교정\n",
    "from hanspell import spell_checker\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "correction_document = []\n",
    "for i in tqdm(range(train_data.shape[0])):\n",
    "    try:\n",
    "        cor_doc = spell_checker.check(train_data['document'][i])\n",
    "        correction_document.append(cor_doc.checked)\n",
    "    except:\n",
    "        correction_document.append(\"\")\n",
    "    # 106분 소요\n",
    "\n",
    "train_data['document'] = pd.DataFrame(correction_document)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [1:41:32<00:00,  8.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# 맞춤법 교정 - 계속\n",
    "from tqdm import tqdm        # p-var\n",
    "\n",
    "from hanspell import spell_checker\n",
    "\n",
    "correction_document = []\n",
    "for i in tqdm(range(test_data.shape[0])):\n",
    "    try:\n",
    "        cor_doc = spell_checker.check(test_data['document'][i])\n",
    "        correction_document.append(cor_doc.checked)\n",
    "    except:\n",
    "        correction_document.append(\"\")\n",
    "    # 35분 소요\n",
    "\n",
    "test_data['document'] = pd.DataFrame(correction_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Id 별 중복되는 document 확인 및 삭제\n",
    "# 다만, id는 다르지만 중복된 리뷰라면 개개인의 의견이니 제거 하지 않는다.\n",
    "train_data[\"id\"].is_unique        # is_unique ; res => boolean True - 중복 없음 / False - 중복 있음  # True\n",
    "test_data['id'].is_unique         # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일을 생성하였습니다.\n",
      "파일을 생성하였습니다.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "## train_data 진행\n",
    "file = 'cor_train_data.txt'\n",
    "\n",
    "if os.path.isfile(file):\n",
    "    print(f'{file} 파일이 존재합니다. 불러옵니다.')\n",
    "    with open('cor_train_data.txt', 'rb') as f:\n",
    "        cor_train_data = pickle.load(f)\n",
    "        train_data['document'] = pd.DataFrame(cor_train_data)\n",
    "        print(\"파일을 불러왔습니다.\")\n",
    "else:\n",
    "    with open(\"cor_train_data.txt\", 'wb') as F:\n",
    "        pickle.dump(correction_document, F, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"파일을 생성하였습니다.\")\n",
    "        \n",
    "        \n",
    "## test_data 진행\n",
    "file = 'cor_test_data.txt'\n",
    "\n",
    "if os.path.isfile(file):\n",
    "    print(f'{file} 파일이 존재합니다. 불러옵니다.')\n",
    "    with open('cor_test_data.txt', 'rb') as f:\n",
    "        cor_test_data = pickle.load(f)\n",
    "        test_data['document'] = pd.DataFrame(cor_test_data)\n",
    "        print(\"파일을 불러왔습니다.\")\n",
    "else:\n",
    "    with open(\"cor_test_data.txt\", 'wb') as F:\n",
    "        pickle.dump(correction_document, F, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"파일을 생성하였습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    0\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측값(NaN, Null) 확인\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값(NaN, Null) 제거\n",
    "train_data.dropna(inplace = True)\n",
    "test_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>4221289</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>9509970</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>10147571</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>1600635</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>7425748</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149445</th>\n",
       "      <td>7133917</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149630</th>\n",
       "      <td>3508604</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149718</th>\n",
       "      <td>7690797</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149773</th>\n",
       "      <td>9233162</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149862</th>\n",
       "      <td>7175749</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1474 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id document  label\n",
       "404      4221289               0\n",
       "412      9509970               1\n",
       "470     10147571               1\n",
       "668      1600635               0\n",
       "972      7425748               0\n",
       "...          ...      ...    ...\n",
       "149445   7133917               0\n",
       "149630   3508604               0\n",
       "149718   7690797               1\n",
       "149773   9233162               0\n",
       "149862   7175749               0\n",
       "\n",
       "[1474 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈값 확인 \n",
    "train_drop = [ i for i, v in enumerate(train_data['document']) if str(v).strip() == \"\" ]\n",
    "test_drop = [ i for i, v in enumerate(test_data['document']) if str(v).strip() == \"\" ]\n",
    "\n",
    "train_data.loc[train_data.index[train_drop]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈값 제거\n",
    "train_data.drop(train_drop, inplace=True)\n",
    "test_data.drop(test_drop, inplace=True)\n",
    "\n",
    "# 인덱스 재정렬\n",
    "train_data.reset_index(drop=True,  inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148526, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NXWvD_4-xJ6"
   },
   "source": [
    "# 3. 데이터 코드화\n",
    "\n",
    "(LSTM 분석을 위해서는 벡터화된 데이터셋이 필요함.)</br>\n",
    "토큰화는 Kkma, Komoran 두가지로 테스트 예정</br>\n",
    "둘 중 프로젝트에 더 적합한 모습을 보이는 라이브러리로 진행을 위한 테스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNpi6jEtIRGD",
    "outputId": "306e641a-9f58-40e5-ed0f-2038a19bb168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "아 더빙 진짜 짜증 나네요 목소리\n",
      "Kkma ['아', '아', '더빙', '진짜', '짜증', '나', '네요', '목소리']\n",
      "Komran ['아', '더빙', '진짜', '짜증', '나', '네요', '목소리']\n",
      "Okt ['아', '더빙', '진짜', '짜증', '나네요', '목소리']\n",
      "Hannanum ['아', '더빙', '진짜', '짜증', '나', '이', '네', '요', '목소리']\n",
      "====================================================================================================\n",
      "흠포스터보고 초등학생영화 줄 오버 연기조차 가볍지 않구나\n",
      "Kkma ['흠', '포스터', '보고', '초등학생', '영화', '줄', '오버', '연기', '조차', '가볍', '지', '않', '구나']\n",
      "Komran ['흠', '포스터', '보고', '초등학생', '영화', '주', 'ㄹ', '오버', '연기', '조차', '가볍', '지', '않', '구나']\n",
      "Okt ['흠', '포스터', '보고', '초등학생', '영화', '줄', '오버', '연기', '조차', '가볍지', '않구나']\n",
      "Hannanum ['흠포스터보', '이', '고', '초등학생영화', '주', 'ㄹ', '오버', '연기', '조차', '가볍', '지', '않', '구나']\n",
      "====================================================================================================\n",
      "너무재밓었다그래서보는것을추천한다\n",
      "Kkma ['너무', '재', '밓', '어', '었', '다', '그래서', '보', '는', '것', '을', '추천', '하', 'ㄴ다']\n",
      "Komran ['너무재밓었다그래서보는것을추천한다']\n",
      "Okt ['너', '무재', '밓었', '다그', '래서', '보는것을', '추천', '한', '다']\n",
      "Hannanum ['너무재밓었다그래서보는것을추천한다']\n",
      "====================================================================================================\n",
      "교도소 이야기구먼 솔직히 재미는 없다 평점 조정\n",
      "Kkma ['교도소', '이야기', '구', '멀', 'ㄴ', '솔직히', '재미', '는', '없', '다', '평점', '조정']\n",
      "Komran ['교도소', '이야기', '이', '구먼', '솔직히', '재미', '는', '없', '다', '평점', '조정']\n",
      "Okt ['교도소', '이야기', '구먼', '솔직히', '재미', '는', '없다', '평점', '조정']\n",
      "Hannanum ['교도소', '이야기구먼', '솔직히', '재미', '는', '없', '다', '평점', '조정']\n"
     ]
    }
   ],
   "source": [
    "import jpype\n",
    "\n",
    "from konlpy.tag import Kkma, Hannanum, Komoran, Okt\n",
    "\n",
    "Kkma = Kkma()\n",
    "Komoran = Komoran()\n",
    "Okt = Okt()\n",
    "Hannanum = Hannanum()\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"=\"*100)\n",
    "    print(train_data['document'][i])\n",
    "    print(\"Kkma\", Kkma.morphs(train_data['document'][i]))\n",
    "    print(\"Komran\",Komoran.morphs(train_data['document'][i]))\n",
    "    print(\"Okt\", Okt.morphs(train_data['document'][i], norm=True))   # norm: 정규화 처리 / stem: 각 단어에서 어간을 추출\n",
    "    print(\"Hannanum\", Hannanum.morphs(train_data['document'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mWta1oTTXKQ"
   },
   "source": [
    "_테스트 결과_<br>\n",
    "맞춤법과 띄어쓰기가 보정된 문장을 단어로 분리하는 경우 전체적으로 비슷한 성능을 보임<br>\n",
    "\n",
    "\n",
    "다만 속도에서 [독보적으로 빠른](https://mr-doosun.tistory.com/22) **Okt**를 사용하는 것이 적절하다고 판단 \n",
    "\n",
    "\n",
    "\n",
    "## 문장 → 단어로 분할 / 불용어 사전 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8bU5uzZImVKT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 148526/148526 [06:12<00:00, 398.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련데이터 에러 발생 수: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import jpype\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','걍','과', '의', '도','을', '것', '를','으로','자','에','와','한','하다',\n",
    "             'ㄴ', '다','에서', '하는', '나', '자', '고', '이다','수','데','인데']\n",
    "\n",
    "# 품사 태깅\n",
    "tagged_train_data = []\n",
    "train_error_sentence = []\n",
    "\n",
    "for i in tqdm(range(train_data.shape[0])):\n",
    "    try:\n",
    "        temp_x = Okt.morphs(train_data['document'][i], norm=True)                 # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords]             # 불용어 제거\n",
    "        tagged_train_data.append(temp_x)\n",
    "    except:\n",
    "        tagged_train_data.append([])\n",
    "        train_error_sentence.append(i)\n",
    "\n",
    "print(\"훈련데이터 에러 발생 수:\", len(train_error_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 49483/49483 [02:31<00:00, 327.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련데이터 에러 발생 수: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['의','가','이','은','들','는','걍','과', '의', '도','을', '것', '를','으로','자','에','와','한','하다', \n",
    "             'ㄴ', '다','에서', '하는', '나', '자', '고', '이다','수','데','인데']   \n",
    "\n",
    "# 품사 태깅\n",
    "tagged_test_data = []\n",
    "test_error_sentence = []\n",
    "\n",
    "for i in tqdm(range(test_data.shape[0])):\n",
    "    try:\n",
    "        temp_x = Okt.morphs(test_data['document'][i], norm=True)                 # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords]           # 불용어 제거\n",
    "        tagged_test_data.append(temp_x)\n",
    "    except:\n",
    "        tagged_test_data.append([])\n",
    "        test_error_sentence.append(i)\n",
    "\n",
    "print(\"훈련데이터 에러 발생 수:\", len(test_error_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일을 생성하였습니다.\n",
      "파일을 생성하였습니다.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "\n",
    "# tagged_train_data 저장\n",
    "file = 'tagged_train_data.txt'\n",
    "\n",
    "if os.path.isfile(file):\n",
    "    print('파일이 존재합니다. 불러옵니다')\n",
    "    with open('tagged_train_data.txt', 'rb') as f:\n",
    "        tagged_train_data = pickle.load(f)\n",
    "        print(\"불러오기 완료.\")\n",
    "else:\n",
    "    with open(\"tagged_train_data.txt\", 'wb') as F:\n",
    "        pickle.dump(tagged_train_data, F, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"파일을 생성하였습니다.\")\n",
    "        \n",
    "        \n",
    "# tagged_test_data 저장\n",
    "file = 'tagged_test_data.txt'\n",
    "\n",
    "if os.path.isfile(file):\n",
    "    print('파일이 존재합니다. 불러옵니다')\n",
    "    with open('tagged_test_data.txt', 'rb') as f:\n",
    "        tagged_test_data = pickle.load(f)\n",
    "        print(\"불러오기 완료.\")\n",
    "else:\n",
    "    with open(\"tagged_test_data.txt\", 'wb') as F:\n",
    "        pickle.dump(tagged_test_data, F, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"파일을 생성하였습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기계가 텍스트를 숫자로 처리할 수 있도록 정수 인코딩을 진행\n",
    "<br>\n",
    "다만 태깅한 품사를 모두 사용해서 학습한다면, 1, 2회 사용된 <br>\n",
    "즉 너무 적은 단위로 사용된 단어들까지 학습된다. <br>\n",
    "하면 데이터 분석에 효율성이 떨어질 수 있다. (혹은 과적합)<br>\n",
    "하여 자주 사용된 단어(3회 이상)만 추려서 인공신경망 학습에 사용될 수 있도록<br>\n",
    "3회 이하 사용된 단어들은 제거 한다. \n",
    "\n",
    "#### 3회 이하로 사용된 단어들이 얼마나 있는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선 정수 인코딩 진행\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer_maxlen = Tokenizer()\n",
    "tokenizer_maxlen.fit_on_texts(tagged_train_data)\n",
    "\n",
    "print(tokenizer_maxlen.word_index)  # word_index => {'하': 1, 'ㄴ': 2, '영화': 3, '다': 4, '고': 5, ... ✨자주 사용된 단어가 먼저 나열\n",
    "                                    # word_counts => OrderedDict([('아', 154), ('더빙', 5), ('진짜', 69) ...\n",
    "                                    # word_docs => defaultdict(<class 'int'>, {'목소리': 2, '짜증나': 8, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_maxlen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-eff9708bb23e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtotal_cnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer_maxlen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# 모든 토큰의 수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrare_cnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m                                                \u001b[1;31m# 3번 이하 등장 단어 개수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtotal_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m                                              \u001b[1;31m# 토근화된 단어의 총합\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrare_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m                                               \u001b[1;31m# 토근화된 단어 중 3번 이하 사용된 단어의 총합\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer_maxlen' is not defined"
     ]
    }
   ],
   "source": [
    "threshold = 4\n",
    "total_cnt = len(tokenizer_maxlen.word_index)                # 모든 토큰의 수\n",
    "rare_cnt = 0                                                # 3번 이하 등장 단어 개수\n",
    "total_freq = 0                                              # 토근화된 단어의 총합\n",
    "rare_freq = 0                                               # 토근화된 단어 중 3번 이하 사용된 단어의 총합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer_maxlen.word_counts.items():       # word_counts: 해당 단어의 빈도수를 반환\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if value < threshold:\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 2번 이하인 희귀 단어의 수:', rare_cnt)\n",
    "print(\"전체 품사 집합에서 2회 이하 사용된 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "voca_size = total_cnt - rare_cnt + 1\n",
    "print(f'즉, 총 단어의 수는 {total_cnt}개 / 그 중 2회 이하 사용된 단어의 수는 {rare_cnt}')\n",
    "print(\"2회 이하 사용된 단어를 제거한 == 남길 단어(Embedding)의 사이즈:\", voca_size - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩 할 단어 voca_size 를 확인</n>\n",
    "\n",
    "### *본 데이터 임베딩 진행\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=voca_size)                           # Tokenizer(vaca_size) => 토근화 진행하면서 max_embadding(100, 12, 4 ...)를 설정\n",
    "# 학습 데이터 \n",
    "tokenizer.fit_on_texts(tagged_train_data)                            # fit_on_texts => Updates internal vocabulary based on a list of texts.\n",
    "enbadded_train_data = tokenizer.texts_to_sequences(tagged_train_data)    # texts_to_sequences => Transforms each text in texts to a sequence of integers.\n",
    "\n",
    "# 검증 데이터 \n",
    "tokenizer.fit_on_texts(tagged_test_data)                             # fit_on_texts => Updates internal vocabulary based on a list of texts.\n",
    "embadded_test_data = tokenizer.texts_to_sequences(tagged_test_data)\n",
    "\n",
    "len(enbadded_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트를 위해 데이터 분리 및 임베딩 데이터 추가\n",
    "ex_train_data = train_data\n",
    "ex_test_data = test_data\n",
    "\n",
    "ex_train_data.insert(ex_train_data.shape[1], \"token\", enbadded_train_data)\n",
    "ex_test_data.insert(ex_train_data.shape[1], \"token\", embadded_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_train_data.to_csv(\"ex_train_data.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 리스트 행 확인\n",
    "train_drop = [i for i, v in enumerate(ex_train_data['token']) if v == []]\n",
    "test_drop = [i for i, v in enumerate(ex_test_data['token']) if v == []]\n",
    "\n",
    "print(len(train_drop))\n",
    "print(len(test_drop))\n",
    "print(train_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 리스트 행 제거 및 확인\n",
    "ex_train_data.drop(train_drop, inplace=True)\n",
    "ex_test_data.drop(test_drop, inplace=True)\n",
    "\n",
    "ex_train_data.reset_index(drop=True, inplace=True)\n",
    "ex_test_data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_test_data.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습을 위한 sequence padding 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(l) for l in refine_train_data[\"token\"]))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, refine_train_data[\"token\"]))/len(refine_train_data[\"token\"]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist([len(s) for s in refine_train_data[\"token\"]], bins=60)\n",
    "plt.xlabel('length ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def belong(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n",
    "\n",
    "max_len = 30\n",
    "for max_len in range(30, 60, 5):\n",
    "    belong(max_len, refine_data[\"token\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding 길이 설정 후 => train_x 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "train_x = sequence.pad_sequences(refine_data['token'], maxlen=40)\n",
    "\n",
    "print(train_x[:3], type(traind_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## refine_train_data에서 라벨값을 추려 => train_label 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = refine_data['label'].to_numpy()\n",
    "print(train_label[:3], type(train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_x 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = sequence.pad_sequences(refine_test_data['token'], maxlen=40)\n",
    "print(test_x[:3], type(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## refine_test_data에서 라벨값을 추려 => test_label 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = refine_test_data['label'].to_numpy()\n",
    "print(test_label[:3], type(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(voca_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(train_x, train_label, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
    "\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(test_x, test_label)[1]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZwk3gkZOGkPxFbpky+WNo",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Naver 영화 리뷰 감정분석.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
